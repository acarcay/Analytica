#!/usr/bin/env python3
"""
Haber Aggregation Job
NewsAPI.org'dan haberleri Ã§ekip Firestore'a cache'ler.

KullanÄ±m:
    python fetch_news_job.py          # Normal Ã§alÄ±ÅŸtÄ±rma (cache kontrolÃ¼ yapar)
    python fetch_news_job.py --force  # Cache'i yoksay, zorla gÃ¼ncelle
"""

import sys
import argparse

from services.newsapi_service import NewsApiService, NewsCacheService
from services.rss_service import RssNewsService


def run_news_aggregation(force: bool = False) -> dict:
    """
    NewsAPI.org ve RSS kaynaklarÄ±ndan haberleri Ã§eker, birleÅŸtirir ve Firestore'a cache'ler.
    """
    stats = {
        'categories_updated': 0,
        'total_articles': 0,
        'skipped': 0,
        'errors': 0
    }

    try:
        news_api = NewsApiService()
        rss_service = RssNewsService()
        cache_service = NewsCacheService()
        
        # TÃ¼m kategorileri kontrol et
        categories_to_update = []
        
        for category in NewsApiService.CATEGORY_CONFIG.keys():
            if force or not cache_service.is_cache_valid(category):
                categories_to_update.append(category)
            else:
                print(f"â­ï¸ {category}: Cache geÃ§erli, atlanÄ±yor")
                stats['skipped'] += 1
        
        if not categories_to_update:
            print("\nâœ… TÃ¼m kategoriler gÃ¼ncel, gÃ¼ncelleme gerekmiyor")
            return stats
        
        print(f"\nğŸ“¥ {len(categories_to_update)} kategori gÃ¼ncellenecek...")
        
        # RSS Haberlerini Ã§ek (HÄ±zlÄ± olduÄŸu iÃ§in topluca Ã§ekiyoruz)
        print("\nğŸ“¡ RSS Haberleri toplanÄ±yor...")
        rss_news_map = rss_service.fetch_all_rss_news()
        
        # Haberleri Ã§ek ve kaydet
        for category in categories_to_update:
            print(f"\nğŸ”„ Kategori iÅŸleniyor: {category}")
            
            # 1. NewsAPI'den Ã§ek
            api_articles = news_api.fetch_news(category)
            
            # 2. RSS'den bu kategoriye ait olanlarÄ± al
            rss_articles = rss_news_map.get(category, [])
            if rss_articles:
                print(f"   â• RSS'den eklenen: {len(rss_articles)} haber")
            
            # 3. BirleÅŸtir ve Deduplicate et
            all_articles = []
            seen_urls = set()
            
            # Ã–nce RSS haberlerini ekle (daha gÃ¼ncel olabilirler)
            for article in rss_articles:
                url = article.get('url')
                if url and url not in seen_urls:
                    all_articles.append(article)
                    seen_urls.add(url)
            
            # Sonra API haberlerini ekle
            dummy_count = 0
            for article in api_articles:
                url = article.get('url')
                if url and url not in seen_urls:
                    all_articles.append(article)
                    seen_urls.add(url)
                else:
                    dummy_count += 1
            
            if dummy_count > 0:
                 print(f"   ğŸ—‘ï¸ {dummy_count} mÃ¼kerrer haber Ã§Ä±karÄ±ldÄ±")

            if all_articles:
                # Tarihe gÃ¶re yeniden sÄ±rala (en yeni en Ã¼stte)
                # Basit string karÅŸÄ±laÅŸtÄ±rmasÄ± yeterli olmayabilir ama format ISO ise Ã§alÄ±ÅŸÄ±r.
                # Emin olmak iÃ§in reverse yapmÄ±yoruz, zaten kaynaklar sÄ±ralÄ± dÃ¶nÃ¼yor.
                # Ancak birleÅŸtirme sonrasÄ± sÄ±ralamak iyi olur.
                try:
                    all_articles.sort(key=lambda x: x.get('publishedAt', ''), reverse=True)
                except:
                    pass # SÄ±ralama hatasÄ± olursa olduÄŸu gibi bÄ±rak
                
                if cache_service.save_news(category, all_articles):
                    stats['categories_updated'] += 1
                    stats['total_articles'] += len(all_articles)
                    print(f"   âœ… Kaydedilen toplam: {len(all_articles)}")
                else:
                    stats['errors'] += 1
            else:
                print(f"   âš ï¸ HiÃ§ haber bulunamadÄ±")
                stats['errors'] += 1
        
        print("\n" + "=" * 50)
        print("ğŸ“Š SONUÃ‡:")
        print(f"   âœ… GÃ¼ncellenen: {stats['categories_updated']} kategori")
        print(f"   ğŸ“° Toplam haber: {stats['total_articles']}")
        print(f"   â­ï¸ Atlanan: {stats['skipped']}")
        print(f"   âŒ Hata: {stats['errors']}")
        
        return stats

    except Exception as e:
        print(f"âŒ Haber toplama sÄ±rasÄ±nda kritik hata: {e}")
        stats['errors'] += 1
        return stats


def main():
    parser = argparse.ArgumentParser(
        description='NewsAPI.org haberlerini Ã§ekip Firestore\'a cache\'ler'
    )
    parser.add_argument(
        '--force', '-f',
        action='store_true',
        help='Cache kontrolÃ¼ yapma, tÃ¼m kategorileri zorla gÃ¼ncelle'
    )
    
    args = parser.parse_args()
    
    try:
        stats = run_news_aggregation(force=args.force)
        
        # Exit code: hata varsa 1, yoksa 0
        if stats.get('errors', 0) > 0:
            sys.exit(1)
        sys.exit(0)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan iptal edildi")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ Kritik hata: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
