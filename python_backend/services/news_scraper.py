"""
Haber Scraping ModÃ¼lÃ¼
Google News ve BeautifulSoup4 kullanarak haber Ã§ekme servisi.
"""

import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import requests
from bs4 import BeautifulSoup

try:
    from GoogleNews import GoogleNews
    GOOGLE_NEWS_AVAILABLE = True
except ImportError:
    GOOGLE_NEWS_AVAILABLE = False
    print("âš ï¸ GoogleNews paketi bulunamadÄ±. pip install GoogleNews ile yÃ¼kleyin.")


@dataclass
class NewsItem:
    """Ham haber verisi."""
    title: str
    url: str
    source: Optional[str] = None
    date: Optional[str] = None
    description: Optional[str] = None
    content: Optional[str] = None  # Sayfa iÃ§eriÄŸi (scraping sonrasÄ±)


class NewsScraper:
    """Google News ve web scraping servisi."""
    
    def __init__(self, language: str = 'tr', region: str = 'TR'):
        """
        Scraper'Ä± initialize et.
        
        Args:
            language: Haber dili (varsayÄ±lan: TÃ¼rkÃ§e)
            region: BÃ¶lge kodu (varsayÄ±lan: TÃ¼rkiye)
        """
        self.language = language
        self.region = region
        self.request_delay = 1.0  # Ä°stekler arasÄ± bekleme sÃ¼resi (saniye)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
        }
    
    def search_news_for_mp(
        self, 
        mp_name: str, 
        max_results: int = 10,
        period: str = '7d'
    ) -> List[NewsItem]:
        """
        Milletvekili iÃ§in Google News'te haber ara.
        
        Args:
            mp_name: Milletvekili adÄ±
            max_results: Maksimum sonuÃ§ sayÄ±sÄ±
            period: Zaman aralÄ±ÄŸÄ± ('1d', '7d', '1m', '1y')
            
        Returns:
            List[NewsItem]: Bulunan haberler
        """
        if not GOOGLE_NEWS_AVAILABLE:
            print("âš ï¸ GoogleNews paketi mevcut deÄŸil. SimÃ¼le edilmiÅŸ veri dÃ¶ndÃ¼rÃ¼lÃ¼yor.")
            return self._get_simulated_news(mp_name, max_results)
        
        try:
            googlenews = GoogleNews(lang=self.language, region=self.region)
            googlenews.set_period(period)
            googlenews.get_news(mp_name)
            
            results = googlenews.results()
            news_items = []
            
            for item in results[:max_results]:
                news_item = NewsItem(
                    title=item.get('title', ''),
                    url=item.get('link', ''),
                    source=item.get('media', ''),
                    date=item.get('date', ''),
                    description=item.get('desc', ''),
                )
                news_items.append(news_item)
            
            googlenews.clear()
            return news_items
            
        except Exception as e:
            print(f"âŒ Google News arama hatasÄ± ({mp_name}): {str(e)}")
            return self._get_simulated_news(mp_name, max_results)
    
    def scrape_article_content(self, url: str) -> Optional[str]:
        """
        Haber URL'sinden makale iÃ§eriÄŸini Ã§ek.
        
        Args:
            url: Haber URL'si
            
        Returns:
            str veya None: Makale metni
        """
        try:
            time.sleep(self.request_delay)  # Rate limiting
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            
            # Ortak iÃ§erik containerlarÄ±nÄ± ara
            content_selectors = [
                'article',
                '.article-content',
                '.post-content',
                '.entry-content',
                '.content-body',
                '.story-body',
                '[itemprop="articleBody"]',
                '.news-content',
                '.haberMetni',  # TÃ¼rk haber siteleri iÃ§in
                '.detay-icerik',
            ]
            
            article_text = ""
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    # Script ve style etiketlerini kaldÄ±r
                    for script in element(['script', 'style', 'aside', 'nav']):
                        script.decompose()
                    
                    paragraphs = element.find_all('p')
                    article_text = ' '.join([p.get_text().strip() for p in paragraphs])
                    
                    if len(article_text) > 100:  # En az 100 karakter
                        break
            
            # EÄŸer hala iÃ§erik bulunamadÄ±ysa, tÃ¼m paragraflarÄ± dene
            if len(article_text) < 100:
                all_paragraphs = soup.find_all('p')
                article_text = ' '.join([p.get_text().strip() for p in all_paragraphs[:20]])
            
            return article_text.strip() if article_text else None
            
        except Exception as e:
            print(f"âš ï¸ Makale scraping hatasÄ± ({url}): {str(e)}")
            return None
    
    def search_and_scrape(
        self, 
        mp_name: str, 
        max_results: int = 5,
        scrape_content: bool = True
    ) -> List[NewsItem]:
        """
        Haber ara ve iÃ§eriklerini Ã§ek.
        
        Args:
            mp_name: Milletvekili adÄ±
            max_results: Maksimum sonuÃ§ sayÄ±sÄ±
            scrape_content: Ä°Ã§erik scraping yapÄ±lsÄ±n mÄ±
            
        Returns:
            List[NewsItem]: Ä°Ã§erikleri Ã§ekilmiÅŸ haberler
        """
        news_items = self.search_news_for_mp(mp_name, max_results)
        
        if scrape_content:
            for i, item in enumerate(news_items):
                print(f"  ğŸ“° Scraping {i+1}/{len(news_items)}: {item.title[:50]}...")
                item.content = self.scrape_article_content(item.url)
        
        return news_items
    
    def _get_simulated_news(self, mp_name: str, count: int = 5) -> List[NewsItem]:
        """
        Test amaÃ§lÄ± simÃ¼le edilmiÅŸ haber verisi dÃ¶ndÃ¼r.
        
        Args:
            mp_name: Milletvekili adÄ±
            count: Haber sayÄ±sÄ±
            
        Returns:
            List[NewsItem]: SimÃ¼le edilmiÅŸ haberler
        """
        simulated_titles = [
            f"{mp_name}, yeni kanun teklifini meclise sundu",
            f"{mp_name}'den ekonomi politikalarÄ±na sert eleÅŸtiri",
            f"TBMM'de {mp_name} ile ilgili Ã¶nemli geliÅŸme",
            f"{mp_name}, seÃ§im bÃ¶lgesinde halkla buluÅŸtu",
            f"{mp_name}'nin sosyal medya paylaÅŸÄ±mÄ± gÃ¼ndem oldu",
            f"Komisyonda {mp_name}'nin Ã¶nerisi kabul edildi",
            f"{mp_name}: 'Reform ÅŸart'",
            f"{mp_name} basÄ±n toplantÄ±sÄ± dÃ¼zenledi",
        ]
        
        news_items = []
        for i in range(min(count, len(simulated_titles))):
            news_items.append(NewsItem(
                title=simulated_titles[i],
                url=f"https://example.com/haber/{i+1}",
                source="SimÃ¼le Haber KaynaÄŸÄ±",
                date=datetime.now().strftime("%Y-%m-%d"),
                description=f"{mp_name} hakkÄ±nda Ã¶nemli geliÅŸmeler...",
                content=f"Bu {mp_name} hakkÄ±nda simÃ¼le edilmiÅŸ bir haber iÃ§eriÄŸidir. "
                        f"GerÃ§ek haberler Ã§ekilemediÄŸinde test amaÃ§lÄ± kullanÄ±lÄ±r. "
                        f"Milletvekili {mp_name}, son dÃ¶nemde aktif bir ÅŸekilde "
                        f"siyasi Ã§alÄ±ÅŸmalarÄ±nÄ± sÃ¼rdÃ¼rmektedir."
            ))
        
        return news_items


# Singleton instance
_scraper_instance: Optional[NewsScraper] = None


def get_news_scraper() -> NewsScraper:
    """NewsScraper singleton instance dÃ¶ndÃ¼r."""
    global _scraper_instance
    if _scraper_instance is None:
        _scraper_instance = NewsScraper()
    return _scraper_instance


if __name__ == "__main__":
    # Test
    scraper = get_news_scraper()
    news = scraper.search_news_for_mp("Kemal KÄ±lÄ±Ã§daroÄŸlu", max_results=3)
    
    print(f"\nğŸ“° {len(news)} haber bulundu:")
    for item in news:
        print(f"  - {item.title}")
        print(f"    Kaynak: {item.source}")
        print(f"    URL: {item.url}")
        print()
